{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spark application\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"pyspark-iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 19:41:49 WARN Utils: Your hostname, galaxia-vostro-3520 resolves to a loopback address: 127.0.1.1; using 10.32.9.180 instead (on interface wlp0s20f3)\n",
      "24/04/24 19:41:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/24 19:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 19:42:09 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Creating spark context\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating the table in iceberg parquet format\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"upc\", StringType(), True),\n",
    "    StructField(\"product_type\", StringType(), True),\n",
    "    StructField(\"price_excl_tax\", IntegerType(), True),\n",
    "    StructField(\"price_incl_tax\", IntegerType(), True),\n",
    "    StructField(\"tax\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"availability\", IntegerType(), True),\n",
    "    StructField(\"num_reviews\", IntegerType(), True),\n",
    "    StructField(\"stars\", IntegerType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "books_df = spark.read.json('books.json', schema)\n",
    "books_df.write.mode(saveMode='overwrite').parquet('data/iceberg/books')\n",
    "books_df.write.mode(saveMode='overwrite').partitionBy('category').parquet('data/iceberg/books_partitioned_by_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet and create a temporary view for querying results\n",
    "books = spark.read.parquet('data/iceberg/books')\n",
    "\n",
    "books.createOrReplaceTempView(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---------------+\n",
      "|               title|stars|       category|\n",
      "+--------------------+-----+---------------+\n",
      "|Scott Pilgrim's l...|    5| sequential art|\n",
      "|Aaron Ledbetter’s...|    5|    young adult|\n",
      "|From a renowned h...|    5|        history|\n",
      "|Punk's raw power ...|    5|          music|\n",
      "|No matter how bus...|    5|        romance|\n",
      "|A Michelin two-st...|    5|        romance|\n",
      "|Anti-apartheid ac...|    5|     nonfiction|\n",
      "|A page-turning no...|    5|     philosophy|\n",
      "|Mark Fallon is an...|    5|       thriller|\n",
      "|In The Four Agree...|    5|   spirituality|\n",
      "|Paris is burning-...|    5|        fiction|\n",
      "|There is a cosmic...|    5|     nonfiction|\n",
      "|On a searing summ...|    5|        fiction|\n",
      "|The Freeman famil...|    5|        fiction|\n",
      "|Slay Procrastinat...|    5|        default|\n",
      "|Change and anger ...|    5|   spirituality|\n",
      "|Just as Annie and...|    5|        fantasy|\n",
      "|THE LONG-AWAITED ...|    5| sequential art|\n",
      "|What if you could...|    5|science fiction|\n",
      "|Tired of the pace...|    5|     nonfiction|\n",
      "+--------------------+-----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read five stars books from parquet\n",
    "five_stars_books_df = spark.sql(\"SELECT title, stars, category FROM books WHERE stars > 4 ORDER BY stars DESC\")\n",
    "\n",
    "five_stars_books_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read books partitioned by category and create a temporary view for querying results\n",
    "books = spark.read.parquet('data/iceberg/books_partitioned_by_category')\n",
    "\n",
    "books.createOrReplaceTempView(\"books_partitioned_by_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n",
      "|               title|stars|category|\n",
      "+--------------------+-----+--------+\n",
      "|The original Psyc...|    5|  horror|\n",
      "|At last—the seque...|    5|  horror|\n",
      "+--------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Querying 5 stars horror books from partitioned books table\n",
    "five_stars_horror_books_df = spark.sql(\"\"\"SELECT title, stars, category\n",
    "                                FROM books_partitioned_by_category\n",
    "                                WHERE stars > 4\n",
    "                                  AND category = 'horror'\n",
    "                                ORDER BY stars DESC\"\"\")\n",
    "\n",
    "five_stars_horror_books_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
